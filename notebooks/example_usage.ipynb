{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "For training, we will store the relative path information for each image and its masks within a dataframe with the following structure:\n",
    "\n",
    "|   images                                                                    |   masks                                               |   train  |\n",
    "|-----------------------------------------------------------------------------|-------------------------------------------------------|----------|\n",
    "|   ./data/images_stacked_channels/mx85-nd-acqusition-0-stacked-channels.tif  |   ./data/masks/mx85-nd-acqusition-0-ground-truth.tif  |   TRUE   |\n",
    "|   ./data/images_stacked_channels/mx85-nd-acqusition-1-stacked-channels.tif  |   ./data/masks/mx85-nd-acqusition-1-ground-truth.tif  |   TRUE   |\n",
    "|   ./data/images_stacked_channels/mx85-nd-acqusition-8-stacked-channels.tif  |   ./data/masks/mx85-nd-acqusition-8-ground-truth.tif  |   FALSE  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ctromans/miniconda3/envs/maddox-dbp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\") # Root of repo\n",
    "import unet.utils.data_utils as utils\n",
    "from unet.utils.load_data import CElegansDataset\n",
    "from unet.networks.unet3d import UNet3D\n",
    "from unet.networks.unet3d import SingleConv\n",
    "import sklearn.model_selection\n",
    "from torch.utils.data import DataLoader\n",
    "from unet.utils.loss import WeightedBCELoss, WeightedBCEDiceLoss\n",
    "import unet.augmentations.augmentations as aug\n",
    "import pandas as pd\n",
    "import torch\n",
    "from unet.utils.trainer import RunTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "# Here we can easily change data parameters and model hyperparameters\n",
    "\n",
    "params = {\n",
    "    \"Normalize\": {\"per_channel\": True},\n",
    "    \"RandomContrastBrightness\": {\"p\": 0.5},\n",
    "    \"Flip\": {\"p\": 0.5},\n",
    "    \"RandomRot90\": {\"p\": 0.5, \"channel_axis\": 0},\n",
    "    \"RandomGuassianBlur\": {\"p\": 0.5},\n",
    "    \"RandomGaussianNoise\": {\"p\": 0.5},\n",
    "    \"RandomPoissonNoise\": {\"p\": 0.5},\n",
    "    \"ElasticDeform\": {\"sigma\":10, \"p\":0.5, \"channel_axis\": 0, \"mode\":\"mirror\"},\n",
    "    \"LabelsToEdges\": {\"connectivity\": 2, \"mode\":\"thick\"},\n",
    "    \"EdgeMaskWmap\": {\"edge_multiplier\":2, \"wmap_multiplier\":1, \"invert_wmap\":True},\n",
    "    \"BlurMasks\": {\"sigma\": 2},\n",
    "    \"ToTensor\": {},\n",
    "    \"batch_size\": 1,\n",
    "    \"epochs\": 100,\n",
    "    \"val_split\": 0.2,\n",
    "    \"patch_size\": (24, 200, 200),\n",
    "    \"create_wmap\": True, ##\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"in_channels\": 2,\n",
    "    \"out_channels\": 1,\n",
    "    \"scheduler_factor\": 0.2,\n",
    "    \"scheduler_patience\": 20,\n",
    "    \"scheduler_mode\": \"min\",\n",
    "    \"loss_function\": WeightedBCEDiceLoss,\n",
    "    # \"targets\": [[\"image\"], [\"mask\"]]\n",
    "    \"targets\": [[\"image\"], [\"mask\"], [\"weight_map\"]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = pd.read_csv(\"./data/data_stacked_channels_training.csv\")[0:2]\n",
    "\n",
    "# Create patches of the dataset\n",
    "# This is performed prior to training since weight-map generation online at train time is \n",
    "# computationally slow\n",
    "utils.create_patch_dataset(source_data, patch_size=params[\"patch_size\"], create_wmap=params[\"create_wmap\"])\n",
    "\n",
    "# Load the patch dataframe\n",
    "training_data = pd.read_csv(\"training_data.csv\")\n",
    "\n",
    "# Create the train/val split\n",
    "train_dataset, val_dataset = sklearn.model_selection.train_test_split(\n",
    "        training_data, test_size=params[\"val_split\"]\n",
    "        )\n",
    "\n",
    "# Define the augmentations for training and validation\n",
    "train_transforms = [\n",
    "    aug.Normalize(**params[\"Normalize\"]),\n",
    "    aug.RandomContrastBrightness(**params[\"RandomContrastBrightness\"]),\n",
    "    aug.Flip(**params[\"Flip\"]),\n",
    "    aug.RandomRot90(**params[\"RandomRot90\"]),\n",
    "    aug.RandomGuassianBlur(**params[\"RandomGuassianBlur\"]),\n",
    "    aug.RandomGaussianNoise(**params[\"RandomGaussianNoise\"]),\n",
    "    aug.RandomPoissonNoise(**params[\"RandomPoissonNoise\"]),\n",
    "    aug.ElasticDeform(**params[\"ElasticDeform\"]),\n",
    "    aug.LabelsToEdges(**params[\"LabelsToEdges\"]),\n",
    "    aug.EdgeMaskWmap(**params[\"EdgeMaskWmap\"]),\n",
    "    aug.BlurMasks(**params[\"BlurMasks\"]),\n",
    "    aug.ToTensor()\n",
    "]\n",
    "val_transforms = [\n",
    "    aug.Normalize(**params[\"Normalize\"]),\n",
    "    aug.LabelsToEdges(**params[\"LabelsToEdges\"]),\n",
    "    aug.EdgeMaskWmap(**params[\"EdgeMaskWmap\"]),\n",
    "    aug.BlurMasks(**params[\"BlurMasks\"]),\n",
    "    aug.ToTensor()\n",
    "]\n",
    "\n",
    "train_ds = CElegansDataset(data_csv=train_dataset, transforms=train_transforms, targets=params[\"targets\"], train_val=\"train\")\n",
    "\n",
    "val_ds = CElegansDataset(data_csv=val_dataset, transforms=val_transforms, targets=params[\"targets\"], train_val=\"val\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Find fastest conv\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Create the train and validation data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    pin_memory=True if device == \"cuda\" else False,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "# Don't shuffle validation so you can see how predictions improve over time\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    pin_memory=True if device == \"cuda\" else False,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "data_loader = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "# Load the model\n",
    "model = UNet3D(\n",
    "    in_channels=params[\"in_channels\"], out_channels=params[\"out_channels\"], f_maps=32\n",
    ")\n",
    "\n",
    "model = utils.load_weights(\n",
    "    model, \n",
    "    weights_path=\"./data/pretrained_model/best_checkpoint.pytorch\", \n",
    "    device=\"cpu\", # Load to CPU and convert to GPU later\n",
    "    dict_key=\"state_dict\"\n",
    ")\n",
    "\n",
    "# If you are intending to change the number of input or output channels, or wish to\n",
    "# freeze the UNet encoder/decoder\n",
    "model = utils.set_parameter_requires_grad(model, trainable=True, trainable_layer_name=None) # could be eg. \"encoder\"\n",
    "\n",
    "# Change the number of input channels\n",
    "model.encoders[0].basic_module.SingleConv1 = SingleConv(params[\"in_channels\"], 16)\n",
    "\n",
    "params_to_update = utils.find_parameter_requires_grad(model)\n",
    "\n",
    "# Send the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Get the loss function, optimizer and scheduler\n",
    "loss_function = params[\"loss_function\"]()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=params[\"scheduler_mode\"], factor=params[\"scheduler_factor\"], patience=params[\"scheduler_patience\"]\n",
    ")\n",
    "\n",
    "# Instantiate the trainer class\n",
    "trainer = RunTraining(\n",
    "    model,\n",
    "    device,\n",
    "    data_loader,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=params[\"epochs\"],\n",
    ")\n",
    "\n",
    "# Run training/validation\n",
    "trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "For running inference we rely on Monai's sliding_window_inference, which enables us to use a model that has been trained with patches to generate predictions for a larger image.\n",
    "\n",
    "The dataframe should be like this:\n",
    "\n",
    "|   images                                                                    |   masks                                               |   train  |\n",
    "|-----------------------------------------------------------------------------|-------------------------------------------------------|----------|\n",
    "|   ./data/images_stacked_channels/mx85-nd-acqusition-x-stacked-channels.tif  |   ./data/masks/mx85-nd-acqusition-x-ground-truth.tif  |   TRUE   |\n",
    "|   ./data/images_stacked_channels/mx85-nd-acqusition-y-stacked-channels.tif  |   ./data/masks/mx85-nd-acqusition-y-ground-truth.tif  |   TRUE   |\n",
    "|   ./data/images_stacked_channels/mx85-nd-acqusition-z-stacked-channels.tif  |   ./data/masks/mx85-nd-acqusition-z-ground-truth.tif  |   FALSE  |\n",
    "|   ./data/images_stacked_channels/mx85-nd-acqusition-a-stacked-channels.tif  |                                                       |          |\n",
    "|   ./data/images_stacked_channels/mx85-nd-acqusition-b-stacked-channels.tif  |                                                       |          |\n",
    "\n",
    "Only images marked with train FALSE will be used for inference. If an image has an associated mask file, inference statistics will be calculated (eg. F1-IoU at multiple thresholds).\n",
    "\n",
    "Below, `predict_from_csv` will get the edge prediction and then perform instance segmentation. The output images will be saved into a folder named `output`. Paths to these images will also be saved in `./output/inference_data.csv`, along with performance test statistics if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_inference = pd.read_csv(\"data/data_test_stacked_channels.csv\")\n",
    "\n",
    "model = UNet3D(\n",
    "    in_channels=2, out_channels=1, f_maps=32\n",
    ")\n",
    "\n",
    "model = utils.load_weights(\n",
    "    model, \n",
    "    weights_path=\"best_checkpoint.pytorch\", \n",
    "    device=\"cpu\", # Load to CPU and convert to GPU later\n",
    "    dict_key=\"state_dict\"\n",
    ")\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Instantiate the inferer class\n",
    "infer = Inferer(\n",
    "    model=model, \n",
    "    patch_size=params[\"patch_size\"],\n",
    "    )\n",
    "\n",
    "# Run inference on csv images\n",
    "infer.predict_from_csv(load_data_inference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maddox-dbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "075cd4bb222163fe48208781b096772c7401b0a6e7eac05801f0c77158161f08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from unet.networks.unet3d import UNet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_data(data_size: int, data_shape: tuple, num_classes: int):\n",
    "    assert len(data_shape) == 4, \"data_shape must have shape (channels, depth, height, width)\"\n",
    "    channels, depth, height, width = data_shape\n",
    "    torch.manual_seed(42)\n",
    "    data_X = []\n",
    "    data_y = []\n",
    "    for i in range(data_size):\n",
    "        # dtype float32 for input pixels\n",
    "        data_X.append(\n",
    "            np.random.rand(channels, depth, height, width).astype(np.float32)\n",
    "        )\n",
    "        # dtype int64 since class predictions are ints\n",
    "        # max = 2 since randint requires it to be one above max. Unsure why\n",
    "        data_y.append(\n",
    "            np.random.randint(0, 2, (num_classes, depth, height, width)).astype(np.int64)\n",
    "        )\n",
    "    return data_X, data_y\n",
    "\n",
    "\n",
    "o, t = build_data(3, (1, 10, 64, 64), 2)\n",
    "len(o), len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mp = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "mp = nn.AvgPool3d(kernel_size=(2, 2, 2), stride=2)\n",
    "x = torch.randn(1, 3, 1024, 1024)\n",
    "mp(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 24, 64, 64]),\n",
       " torch.Size([1, 2, 24, 64, 64]),\n",
       " torch.Size([1, 2, 24, 64, 64]))"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = UNet3D(input_channels=1, num_classes=2, dropout_probability=0.1, conv_kernel_size=3, activation=\"sigmoid\")\n",
    "img_shape = (1, 24, 64, 64)\n",
    "x = torch.randn(1, *img_shape)\n",
    "y = torch.randint(0, 2, (pred.shape), dtype=torch.long)\n",
    "pred = unet(x)\n",
    "x.shape, y.shape, pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0]), array([ True,  True,  True]), 0.3333333333333333, 1.0)"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "probability = pred.sigmoid().detach().numpy().flatten()[0:3]\n",
    "y = y.long().detach().cpu().numpy().flatten()[0:3]\n",
    "\n",
    "# roc = sklearn.metrics.roc_auc_score(y, probability, labels=[0, 1], multi_class=\"ovo\")\n",
    "\n",
    "# Convert probabilities into binary predictions\n",
    "th_prediction = (probability > 0.5)\n",
    "\n",
    "y[0:3], th_prediction[0:3], sklearn.metrics.precision_score(y, th_prediction), sklearn.metrics.recall_score(y, th_prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you have a two class problem. Your input GT mask has two one-hot encoded classes, the cell membrane and the cell center.\n",
    "\n",
    "Questions: \n",
    "1. If using softmax activation, how do you translate these into multiclass readouts?\n",
    "   1. How do you determine that a pixel is classified as one, but not the other?\n",
    "      1. Argmax?\n",
    "2. For sigmoid activation, you are assuming that some pixels can be multiple classes. Explore this. \n",
    "\n",
    "\n",
    "If you intend to use Cross Entropy Loss, it already includes a softmax, so it's likely a bad idea to include it in the model. \n",
    "\n",
    "I think that using sigmoid makes sense. Since you are predicting cell boundaries, they will overlap with one another. You'll perform watershed on them later. So, perform sigmoid on the outputs, then **threshold** the outputs with a value of **0.5**\n",
    "   0.5 is used since it's a binary threshold problem and sigmoid scales between 0-1. If you were using softmax for 4 classes, the thresholds for each class would be (1/4), so 0.25 for each class. Ie. if a pixel was between 0-0.25, it'd be class 1, 0.25-0.5 is class 2 etc. \n",
    "\n",
    "We will predict two classes this way using sigmoid since we used the data augmentation to calculate cell edges and centers in a one-hot encoded manner (mask shape of (class, depth, height, width)). \n",
    "\n",
    "Try to take the maddox dataset and make it much, much smaller, just so we can run some local tests. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two one-hot encoded 3d images with size 10x10x10\n",
    "img_shape = (1, 10, 10, 10)\n",
    "input_img = torch.randn(1, *img_shape)\n",
    "gt_mask = np.zeros((2, 10, 10, 10))\n",
    "\n",
    "gt_mask[0, 0:5, 0:5, 0:5] = 1\n",
    "gt_mask[1, 5:, 5:, 5:] = 1\n",
    "\n",
    "unet_none = UNet3D(input_channels=1, num_classes=3, network_depth=2, dropout_probability=0.1, conv_kernel_size=3, activation=None)\n",
    "unet_sig = UNet3D(input_channels=1, num_classes=3, network_depth=2, dropout_probability=0.1, conv_kernel_size=3, activation=\"sigmoid\")\n",
    "unet_soft = UNet3D(input_channels=1, num_classes=3, network_depth=2, dropout_probability=0.1, conv_kernel_size=3, activation=\"softmax\")\n",
    "\n",
    "pred_none = unet_none(input_img).detach().numpy()[0,...]\n",
    "pred_sig = unet_sig(input_img).detach().numpy()[0,...]\n",
    "pred_soft = unet_soft(input_img).detach().numpy()[0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.36745355, 0.28825787, 0.35672456], dtype=float32),\n",
       " array([0.37790936, 0.40899363, 0.35661   ], dtype=float32),\n",
       " array([0.25463706, 0.3027485 , 0.28666544], dtype=float32))"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_soft[0, 0, 0, 0:3], pred_soft[1, 0, 0, 0:3], pred_soft[2, 0, 0, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rw/691cc6h16f9cfxlh9g6bpq400000gq/T/ipykernel_71569/3590863840.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nn.Softmax(dim=None)(torch.tensor([0, 0.1, 0.2]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3006, 0.3322, 0.3672])"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=None)(torch.tensor([0, 0.1, 0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29dafc760>"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAACXCAYAAABJNBKHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAQJklEQVR4nO3df2zc9X3H8df5fHeOHduBBRxbcU1IG5IGAyVAsNtM2UDeDNE2kkBCJYSm0CnLNOYGNsiiKhbTlLVoLG1FUAl0TaSyRO20ahWpMsNKxkjDj9QhiGTFxGG265hgU2wT22f77rs/wOlMbO79tT/nr+/L8yGdRE5vv+/z/d6Lr9+5u3wu4nmeJwAAAAfygl4AAAAIDwYLAADgDIMFAABwhsECAAA4w2ABAACcYbAAAADOMFgAAABn8mf6AdPptDo7O1VcXKxIJDLTDw8fPM9Tf3+/KioqlJfnbgYlA7klGzkgA7mFawEkew5mfLDo7OxUZWXlTD8spqG9vV0LFy501o8M5CaXOSADuYlrAaTMOZjxwaK4uFiS9L+/vEIlc8P1TswdS6qDXoJToxrRf+vghefMlbF+X9FtylfMaW+4l40cjPX6/FMNihYmMtanXi819y5817aZ8KXrOsw9T79VYa5dcc1pc21zm/2X9NyipKku8tw8c8/RW3pNdamBpFo2fTtr14Lqu76haKwgY/28jfbn7Ncv2AaWmjWvm3se/+drzLVX3tNirj3x4hfMtbfc0myqW11yytxz22trTXXpwaQ67v9WxhxMabDYvXu3Hn30UZ09e1bLly/Xrl27tGrVKtPPjr3cVTI3TyXF4Ros8iMh+yX58fV5opcoXWQgX7HwnbMwykIOxnpFCxOmwUKJzL90xkTjtsEiv8jwuB/Lm2N//FhR3N630MdxFdrqInF7T69wyFwrZe9aEI0VKGpYt5/nLGrMTHyu/fmyrHGMrxwU2PvG59qumYXFUfvj+8ihNHEOxvXz1U3SgQMH1NDQoO3bt6u5uVmrVq1SfX292tra/LZCjiIDkMgByAAm5nuweOyxx7Rp0ybdd999WrZsmXbt2qXKyko98cQT2VgfZiEyAIkcgAxgYr4Gi+HhYR07dkx1dXXj7q+rq9ORI0cm/JlkMqm+vr5xN+QuMgDJfw7IQPhwLcBkfA0W3d3dSqVSKisrG3d/WVmZurq6JvyZnTt3qrS09MKNTwDnNjIAyX8OyED4cC3AZKb06clPfnDD87xJP8yxbds29fb2Xri1t7dP5SExy5ABSPYckIHw4lqAT/L1r0Lmz5+vaDR60TR67ty5i6bWMYlEQomE/ZO8mN3IACT/OSAD4cO1AJPx9YpFPB7XihUr1NTUNO7+pqYm1dbWOl0YZicyAIkcgAxgcr73sdi6davuuece3XDDDaqpqdGTTz6ptrY2bd68ORvrwyxEBiCRA5ABTMz3YLFhwwb19PTokUce0dmzZ3X11Vfr4MGDqqqqysb6MAuRAUhucnC+q8i0+VTVayPmnm0b0qa6r5adNPfs+MC+8+eW8v801/4wVmOufeUHXzLV9V9h2yBMkkbfnWuqSw9O/KvC1bWg4p5W04ZSx8/YP+xZftqWgyM/sp1XSZozaj+3v/qXpebazx0fMNc2jdxoe/xG+/8zv3OvbYOs1LBk2aFkSjtvbtmyRVu2bJnKjyIkyAAkcgAygIuFa09tAAAQKAYLAADgDIMFAABwhsECAAA4w2ABAACcYbAAAADOMFgAAABnGCwAAIAzDBYAAMCZKe28CQBOzEl9dMvgN0vt34j509X/aKpb87O/MvdctrTDXHvv818z1176qv0S/NS2Xaa6Ow/+pbmnvIm/3nzKdVPU1nuJoiOZn+PCk7atpyXp3ZtsW3ovefRtc8+eP1hsrh0uNpeq+6+HzLWX/tB2Djr+1v5FcINX2R4/PZiUnslcxysWAADAGQYLAADgDIMFAABwhsECAAA4w2ABAACcYbAAAADOMFgAAABnGCwAAIAzDBYAAMAZdt4EEJjYubjyCuIZ6/IHPHPP9a/+malu3pv2y9/Qv5aba+969FVzbVlNn73vT+431f3dbT8y92z8yV2muvRQ1NxzKgabL1U0kXlHySv3njb3jB2w1Z38un03zdE59hyu+fJr5tpk2p7F2//+30113zz9h+aeO6583lQ30J/SJkMdr1gAAABnGCwAAIAzDBYAAMAZBgsAAOAMgwUAAHCGwQIAADjDYAEAAJxhsAAAAM4wWAAAAGcYLAAAgDNs6Q0gMAVdEUUTkYx15yvsPdOpzP0kKTnP3nPNrp+ba7/76u+Za5f86TFz7Y0vvWWq+4enNph7jn4xaapLDw6be07F5cdGlB/LvG14155Sc895jxSb6lK327fpji0YMNe++P0bzbUfVI+aa3/+m+tMddEvfGju+dB/3WmqSw8OSTqesY5XLAAAgDMMFgAAwBkGCwAA4AyDBQAAcIbBAgAAOMNgAQAAnGGwAAAAzjBYAAAAZxgsAACAMwwWAADAmcC29L5jSbXyI7GgHh7IikOdmbe7zTV9/WldsiQ7vavXn1SsKJ6x7sQzV5t7jg7MNdUNlqXNPX/64C3m2jPf32OufajZtj2zJP34sO1JmFPTa+5Z+Jpti+xU0r7t9ZT8RbdUlMhY1vt6ubll1Y63TXWRVxabe3qni8y1eSn7OVv23Q/MtYOVJaa699+zbWkuSXHj/wqppK2QVywAAIAzvgaLxsZGRSKRcbcFCxZka22YhcgAJHIAMoDJ+X4rZPny5Xruuecu/DkazfyNdAgXMgCJHIAMYGK+B4v8/Hym0s84MgCJHIAMYGK+P2PR0tKiiooKLVq0SBs3blRra+un1ieTSfX19Y27IbeRAUj+ckAGwolrASbia7BYuXKl9u3bp0OHDmnPnj3q6upSbW2tenp6Jv2ZnTt3qrS09MKtsrJy2otGcMgAJP85IAPhw7UAk/E1WNTX12vdunWqrq7WrbfeqmeffVaStHfv3kl/Ztu2bert7b1wa29vn96KESgyAMl/DshA+HAtwGSmtY9FUVGRqqur1dLSMmlNIpFQIpH53ycjN5EBSJlzQAbCj2sBxkxrH4tkMqlTp06pvNy+aQnChQxAIgcgA/gtX4PFgw8+qMOHD+vMmTN6+eWXtX79evX19enee+/N1vowy5ABSOQAZACT8/VWSEdHh+6++251d3frsssu080336yjR4+qqqoqW+vDLEMGILnLwYlzFYoWZn5pfN/WfzL33PBMg6nu7t9/ydzz2bdXmWuveeVuc+3wiXnm2oLBiKlu9ZdtW1lL0s/Of9FUlx4Yuug+l9eCoviwYpl3dtecz9u3K2/+nytMdbGFA+aeo+8Wmmtjf9xtru1MXG6uzbt18g/H/n/F8RH74791makuPZgy1fkaLPbv3++nHCFEBiCRA5ABTI7vCgEAAM4wWAAAAGcYLAAAgDMMFgAAwBkGCwAA4AyDBQAAcIbBAgAAOMNgAQAAnGGwAAAAzkzr200BYDpGRqJKj2S+DN15wLZNtyQVfGDb+vqZ128093z+oW+Za285fL+59uvrDpprv/Psbaa6g7+4ztwz0RM11aWGbOd0qubGkorFvIx1Zd8pMPf8cINtzSO99m9bjZ23n4f33ppvrr3q0Dlz7cpNJ011P/jFV8w9z6z/nqmurz+tS/4mcx2vWAAAAGcYLAAAgDMMFgAAwBkGCwAA4AyDBQAAcIbBAgAAOMNgAQAAnGGwAAAAzjBYAAAAZ9h5E0BghgdjylMsY11+1YC55+jitKlubmzU3HPtNw3bDX7szq8dMdc+/b3bzbX/8YBt989v/HqNueeJc+WmuryBpLnnVLx5aImiicy7aqZW23vOPW2rS7yfecfPMe//rv08lF3ea65tu6PMXPsncVu+Ln/JtquqJF3V/eemuvTQkKTtGet4xQIAADjDYAEAAJxhsAAAAM4wWAAAAGcYLAAAgDMMFgAAwBkGCwAA4AyDBQAAcIbBAgAAOMNgAQAAnGFLbwCBibUlFC1IZKyLLu8z9yz5cbGprvu6iLnnl776K3PtH5X+0lz7bwtqzLU7Om8z1R19aZm5Z2qebVvz9GB2f1Usq2tRrCiesW44ZV/H4GjmreIlKZFv39o90l9iru1qu9TetyJlrj3+4edMde/dat9+/LpF7aa6kfPDajXU8YoFAABwhsECAAA4w2ABAACcYbAAAADOMFgAAABnGCwAAIAzDBYAAMAZBgsAAOAMgwUAAHBmxnfe9DxPkjSqEcmb6UeHH6MakfTb58yVMGegrz8d9BKc6/vwo2NymYOxXunkkO0HBuy7CKZGbDsupofsO2+OnB821573kYH0kPH4fazBT8/0oHXnzY96ZutaYD22kZT93I6O2mrz8u27XqZ85HDsnFlEkva/4w9/aMyBj8c3n/+P6zLlIOK5TkoGHR0dqqysnMmHxDS1t7dr4cKFzvqRgdzkMgdkIDdxLYCUOQczPlik02l1dnaquLhYkchHf2Po6+tTZWWl2tvbVVJi34t9tsv14/I8T/39/aqoqFBenrt3zchAbslGDibKgBSO8zWRXD8urgXTF4bjsuZgxt8KycvLm3TSKSkpydkT/mly+bhKS0ud9yQDucd1Dj4tA1Lun6/J5PJxcS1wI9ePy5IDPrwJAACcYbAAAADORBsbGxuDXoQkRaNRrV69Wvn5M/7uTFaF9biyIaznKqzHlS1hPV9hPa5sCOu5CutxfdKMf3gTAACEF2+FAAAAZxgsAACAMwwWAADAGQYLAADgDIMFAABwJvDBYvfu3Vq0aJEKCgq0YsUKvfjii0EvadoaGxsViUTG3RYsWBD0sma1sOWADPhHBiCRgzAIdLA4cOCAGhoatH37djU3N2vVqlWqr69XW1tbkMtyYvny5Tp79uyF2xtvvBH0kmatsOaADNiRAUjkIDS8AN10003e5s2bx923dOlS7+GHHw5oRW7s2LHDu/baa4NeRs4IYw7IgD9kAJ5HDsIisFcshoeHdezYMdXV1Y27v66uTkeOHAloVe60tLSooqJCixYt0saNG9Xa2hr0kmalMOeADNiQAUjkIEwCGyy6u7uVSqVUVlY27v6ysjJ1dXUFtCo3Vq5cqX379unQoUPas2ePurq6VFtbq56enqCXNuuENQdkwI4MQCIHYRL4huWRSGTcnz3Pu+i+XFNfX3/hv6urq1VTU6PFixdr79692rp1a4Arm73ClgMy4B8ZgEQOwiCwVyzmz5+vaDR60SR67ty5iybWXFdUVKTq6mq1tLQEvZRZ57OSAzIwOTIAiRyESWCDRTwe14oVK9TU1DTu/qamJtXW1ga0quxIJpM6deqUysvLg17KrPNZyQEZmBwZgEQOQiXIT47u37/fi8Vi3tNPP+2dPHnSa2ho8IqKirx33nknyGVN2wMPPOC98MILXmtrq3f06FFvzZo1XnFxcc4fV7aEMQdkwB8yAM8jB2ER6GDheZ73+OOPe1VVVV48Hveuv/567/Dhw0Evado2bNjglZeXe7FYzKuoqPDWrl3rvfnmm0Eva1YLWw7IgH9kAJ5HDsIg4nmeF/SrJgAAIBwC39IbAACEB4MFAABwhsECAAA4w2ABAACcYbAAAADOMFgAAABnGCwAAIAzDBYAAMAZBgsAAOAMgwUAAHCGwQIAADjzf8E2qHjX0kzMAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 4)\n",
    "ax[0].imshow(gt_mask[0,0,...])\n",
    "ax[1].imshow(gt_mask[1,5,...])\n",
    "ax[2].imshow(pred_sig[1,5,...])\n",
    "ax[3].imshow(pred_soft[1,5,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5012746 , 0.53113014, 0.56106776, 0.46884644, 0.5585482 ],\n",
       "       dtype=float32),\n",
       " array([0.41740853, 0.5848051 , 0.39384654, 0.59576803, 0.5523354 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sig.flatten()[0:5], pred_soft.flatten()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 6, 32, 32])\n",
      "torch.Size([1, 128, 12, 64, 64])\n",
      "torch.Size([1, 64, 24, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "from unet.networks.unet3d import Encoder\n",
    "\n",
    "encoder = Encoder()\n",
    "# input image\n",
    "# Input shape is (batch, channel, depth, height, width)\n",
    "x = torch.randn(1, 1, 24, 128, 128)\n",
    "ftrs = encoder(x)\n",
    "for ftr in ftrs[::-1][1:]:\n",
    "    print(ftr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 24, 128, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unet.networks.unet3d import Decoder\n",
    "\n",
    "decoder = Decoder()\n",
    "# Input to the decoder does not have the batch dim, just channel\n",
    "x = torch.randn(1, 512, 3, 16, 16)\n",
    "decoder(x, ftrs[::-1][1:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 20, 100, 200])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.ConvTranspose3d(16, 33, kernel_size=2, stride=2)\n",
    "# non-square kernels and unequal stride and with padding\n",
    "# m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))\n",
    "input = torch.randn(20, 16, 10, 50, 100)\n",
    "m(input).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble model training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the number of models to use in the ensemble\n",
    "num_models = 5\n",
    "\n",
    "# Create a list to store the models\n",
    "models = []\n",
    "\n",
    "# Train each model\n",
    "for i in range(num_models):\n",
    "    # Create the model\n",
    "    model = UNet3D(in_channels, out_channels)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()  # Set the model to training mode\n",
    "    # Use a loop to iterate over the training data, and pass the input through the model to compute the loss\n",
    "    # and update the model weights using backpropagation and an optimizer\n",
    "    ...\n",
    "\n",
    "    # Add the trained model to the list\n",
    "    models.append(model)\n",
    "\n",
    "# At test time, to make predictions using the ensemble of models, you can do the following:\n",
    "\n",
    "# Set all models to evaluation mode\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "# Iterate over the test data\n",
    "for input in test_data:\n",
    "    # Create a list to store the predictions of each model\n",
    "    model_predictions = []\n",
    "    # Iterate over the models\n",
    "    for model in models:\n",
    "        # Compute the prediction for the current model\n",
    "        prediction = model(input)\n",
    "        # Add the prediction to the list\n",
    "        model_predictions.append(prediction)\n",
    "    # Combine the predictions of the individual models using an averaging or voting strategy\n",
    "    ensemble_prediction = combine_predictions(model_predictions)\n",
    "    # Use the ensemble prediction for evaluation or further processing\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2872)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating multichannel dice loss\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def dice_loss(self, prediction, target):\n",
    "        # Make arrays 1D\n",
    "        prediction = prediction.flatten()\n",
    "        target = target.flatten()\n",
    "        target = target.float()\n",
    "\n",
    "        intersection = (prediction * target).sum()\n",
    "\n",
    "        loss = 1 - 2 * (intersection) / (prediction.sum() + target.sum())\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, prediction, target):\n",
    "        prediction = nn.Sigmoid()(prediction)\n",
    "\n",
    "        dice = self.dice_loss(prediction, target)\n",
    "\n",
    "        avg_channel_dice = 1. - torch.mean(dice)\n",
    "\n",
    "        return avg_channel_dice\n",
    "        # return dice\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        output = self.bce(prediction, target.float()) + self.dice(prediction, target)\n",
    "\n",
    "        return output\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.rand((2, 24, 128, 128))\n",
    "y = torch.randint(0, 2, (2, 24, 128, 128))\n",
    "\n",
    "# loss = DiceLoss()\n",
    "loss_fn = BCEDiceLoss()\n",
    "\n",
    "loss = loss_fn(x, y)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maddox-dbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "075cd4bb222163fe48208781b096772c7401b0a6e7eac05801f0c77158161f08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
